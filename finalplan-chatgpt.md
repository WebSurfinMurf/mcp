# Refined Plan: LiteLLM v1.77.3 MCP Gateway on linuxserver.lan

> **üö® HARD RULE - LOCAL NETWORK DEPLOYMENT ONLY:**
> **This is a LOCAL NETWORK ONLY deployment using `linuxserver.lan`:**
> - **NEVER use Traefik** reverse proxy or routing
> - **NEVER use Keycloak** authentication or SSO
> - **NEVER use ai-servicers.com** DNS or HTTPS
> - **ALWAYS use direct port access** (e.g., `http://linuxserver.lan:4000`)
> - **ALWAYS use HTTP** (not HTTPS)
>
> **üîí COMPLIANCE RULE:** These directives are HARD RULES and must NOT be violated unless:
> 1. Claude asks about a specific technical issue
> 2. User directly tells Claude to violate this order for that specific issue
>
> All references to Traefik/OIDC/authentication below are for FUTURE consideration only and must be IGNORED for this deployment.

## 0. Community-Supported MCP Gateway Options (Assignment Outcome)
| Candidate | Community Status | MCP Support | Fit for Requirement | Notes |
|-----------|------------------|-------------|---------------------|-------|
| **LiteLLM Proxy** (BerriAI) | 9.5k‚òÖ GitHub, active releases, Discord + docs | Native `mcp_servers` for `stdio`, `http`, `sse`; integrates auth, key mgmt, logging | ‚úÖ Strong match; already part of stack; OpenAI-compatible API for clients (Claude Code, Gemini CLI, ChatGPT Codex, Open WebUI, VS Code) | Runs as central gateway; no code changes required; supports LAN-only deployments |
| **Model Context Runner** (Sourcegraph) | 2.5k‚òÖ GitHub, maintained; designed for Claude Desktop/VS Code | Focused on dev tooling; MCP registry but less customizable for multi-client sharing | ‚ö†Ô∏è Partial fit; better for per-user environments than central LAN service | Requires per-user runtimes; limited routing features |
| **mcp-proxy** (Community forks) | Multiple small repos (<200‚òÖ) | Experimental HTTP/SSE forwarding | ‚ùå Weak fit; low adoption, unclear maintenance | Would require manual auth/logging layers |

**Recommendation:** Use **LiteLLM Proxy** as the central MCP gateway. It has the strongest community backing, already aligns with your infrastructure, and natively supports MCP transports plus the OpenAI-compatible API your clients expect. No code modifications to LiteLLM or third-party MCP servers are necessary‚Äîonly configuration and Docker orchestration.

## 1. Context Snapshot
- Initialization files reviewed per `AINotes/memoryfiles.md` directive.
- Requirements confirmed in `projects/mcp/requirements.md`: LiteLLM lives under `projects/litellm`, individual MCP servers under `projects/mcp/{service}`.
- Infrastructure provides shared Postgres (`postgres-net`), Traefik reverse proxy, centralized observability, and LAN DNS (`*.linuxserver.lan`).
- Prior LiteLLM + MCP attempts failed; MCP directories reset for clean implementation.

## 2. Goals and Guardrails
- Keep deployment LAN-only while enabling future Traefik/OIDC layering.
- Pin LiteLLM to `v1.77.3-stable`, avoid modifying upstream code or MCP packages.
- Standardize registration patterns for `stdio`, `http`, and `sse` transports.
- Follow existing naming conventions and secrets handling (`/home/administrator/secrets/`).
- Provide validation, observability, and rollback guidance.

## 3. Target Architecture Overview
- **LiteLLM project root:** `/home/administrator/projects/litellm`
  - Config: `/home/administrator/projects/litellm/config/config.yaml`
  - Compose file: `/home/administrator/projects/litellm/docker-compose.yml`
  - Scripts/notes: `/home/administrator/projects/litellm/README.md` (update after success)
- **Secrets file:** `/home/administrator/secrets/litellm.env` (600 perms)
- **MCP services:** individual directories under `/home/administrator/projects/mcp/` (e.g., `/home/administrator/projects/mcp/postgres`, `/home/administrator/projects/mcp/filesystem`), each with its own Compose or Dockerfile as needed.
- **Networks:**
  - `traefik-proxy` (external, existing) for future reverse proxy integration.
  - `postgres-net` (external) for database connectivity.
  - `litellm-mcp-net` (new bridge) linking LiteLLM and MCP containers.
- **Core containers:**
  1. `litellm-proxy` (LiteLLM v1.77.3-stable)
  2. Optional `litellm-db` (Postgres 16-alpine) if not using shared cluster.
  3. `mcp-postgres` (community `crystaldba/postgres-mcp`) as first MCP connector.
  4. Additional MCP services (filesystem, fetch, n8n, etc.) deployed in their respective directories and attached to `litellm-mcp-net`.

## 4. MCP Transport Best Practices
| Transport | Usage Guidance | LiteLLM Config Snippet | Operational Notes |
|-----------|----------------|------------------------|-------------------|
| **SSE (preferred)** | Default for containerized MCP services on LAN. Run MCP in its own container, expose fixed port, connect via `litellm-mcp-net`. | ```yaml
mcp_servers:
  db_main:
    transport: sse
    url: http://mcp-postgres:8686
    api_keys: [${LITELLM_VIRTUAL_KEY_TEST}]
``` | Supports long-lived connections, streaming, simple firewalling. Provide health endpoints where available. |
| **HTTP** | Use if MCP server exposes RESTful MCP interface (rare today). | ```yaml
mcp_servers:
  monitoring:
    transport: http
    url: http://mcp-monitor:8700
    auth_type: bearer_token
    auth_value: ${MCP_MONITOR_TOKEN}
``` | Ensure server adheres to MCP HTTP spec; configure auth headers through LiteLLM. |
| **Stdio** | Only for lightweight local tools packaged alongside LiteLLM in `/home/administrator/projects/litellm/tools`. | ```yaml
mcp_servers:
  local_tools:
    transport: stdio
    command: "/app/tools/run"
    args: ["--mode", "cli"]
    env:
      TOOLS_ROOT: /app/tools
``` | Avoid wrapping Docker containers with stdio; instead expose them via SSE. Requires binaries accessible within LiteLLM container image or bind-mount. |

**Auth & Header Tips:**
- Use `api_keys` to scope LiteLLM virtual keys to specific MCP servers.
- Clients select servers via `x-mcp-servers` header (`db_main,fs_local`).
- Per-request headers forwarded via `x-mcp-{alias}-{header}` (e.g., `x-mcp-db_main-authorization`).
- Static secrets can be injected with `auth_type: bearer_token` and `auth_value` placeholder variables that resolve from `litellm.env`.

## 5. Implementation Phases

### Phase A ‚Äì Environment Preparation
1. Ensure project structure:
   ```bash
   cd /home/administrator/projects/litellm
   mkdir -p config tools tmp
   ```
2. Create secrets file `/home/administrator/secrets/litellm.env` containing (replace placeholders):
   ```bash
   LITELLM_MASTER_KEY=sk-...
   DATABASE_URL=postgresql://litellm_user:...@postgres:5432/litellm_db
   LITELLM_VIRTUAL_KEY_TEST=litellm-test-key-...
   ```
   Set permissions `chmod 600 /home/administrator/secrets/litellm.env`.
3. **Database strategy decision:**
   - **Option A (recommended):** Reuse existing Postgres on `postgres-net`; keep `DATABASE_URL` pointed at that hostname.
   - **Option B:** Deploy the optional `litellm-db` service below; update `DATABASE_URL` to reference it.
   Choose one approach before continuing.
4. If using shared Postgres:
   - Follow `AINotes/integration.md` to create `litellm_db` and `litellm_user`.
   - Grant read privileges for MCP connectors as needed; keep write access disabled until explicitly required.
5. Reserve LAN ports (LiteLLM `4000`, MCP connectors `48xxx`) ensuring no collisions (`sudo ss -tlnp` if permissible).
6. Populate `/home/administrator/projects/mcp/postgres/` with connector assets (compose file, docs) for the community `crystaldba/postgres-mcp` image.

### Phase B ‚Äì LiteLLM Docker Compose (`/home/administrator/projects/litellm/docker-compose.yml`)
```yaml
version: "3.9"

services:
  litellm-proxy:
    image: ghcr.io/berriai/litellm:v1.77.3-stable
    container_name: litellm-proxy
    restart: unless-stopped
    command: ["--config", "/app/config/config.yaml", "--detailed_debug"]
    env_file: /home/administrator/secrets/litellm.env
    volumes:
      - ./config:/app/config:ro
    ports:
      - "4000:4000"
    networks:
      - traefik-proxy
      - postgres-net
      - litellm-mcp-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "5"

  # Optional dedicated Postgres for LiteLLM metadata
  litellm-db:
    image: postgres:16-alpine
    container_name: litellm-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${LITELLM_DB_USER}
      POSTGRES_PASSWORD: ${LITELLM_DB_PASSWORD}
      POSTGRES_DB: litellm_db
    volumes:
      - litellm_pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${LITELLM_DB_USER} -d litellm_db"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - litellm-mcp-net
```
Add external network declarations and optional volume:
```yaml
networks:
  traefik-proxy:
    external: true
  postgres-net:
    external: true
  litellm-mcp-net:
    driver: bridge

volumes:
  litellm_pgdata:
```
If relying on shared Postgres, omit `litellm-db` service and ensure `DATABASE_URL` points to existing server on `postgres-net`.

### Phase C ‚Äì LiteLLM Configuration (`config/config.yaml`)
```yaml
litellm_settings:
  master_key: ${LITELLM_MASTER_KEY}
  database_url: ${DATABASE_URL}
  json_logs: true
  mcp_aliases:
    db: db_main

general_settings:
  detailed_debug: true

model_list:
  - model_name: gpt-4o-mock
    litellm_params:
      model: mock-response
      api_key: mock

virtual_keys:
  - api_key: ${LITELLM_VIRTUAL_KEY_TEST}
    models: ["gpt-4o-mock"]
    mcp_servers: ["db"]

mcp_servers:
  db_main:
    transport: sse
    url: http://mcp-postgres:8686
    api_keys: [${LITELLM_VIRTUAL_KEY_TEST}]
    description: "PostgreSQL metadata + health tooling"
    health_check: /health
```
Add stubs for future HTTP/stdio services with comments referencing Section 4 patterns.

### Phase D ‚Äì MCP Connector Deployment (UPDATED: Native SSE Transport)

**BREAKTHROUGH UPDATE**: The `crystaldba/postgres-mcp` container **natively supports SSE transport** via command-line arguments! This eliminates the need for custom HTTP adapters and provides a much cleaner solution.

#### Simple SSE Configuration
In `/home/administrator/projects/mcp/postgres/docker-compose.yml`:
```yaml
version: "3.9"

services:
  mcp-postgres:
    # Use the original image directly, no custom build needed
    image: crystaldba/postgres-mcp:latest
    container_name: mcp-postgres
    restart: unless-stopped
    env_file:
      - /home/administrator/secrets/mcp-postgres.env

    # Enable SSE mode with correct command-line arguments (includes database URL)
    command: ["--transport", "sse", "--sse-host", "0.0.0.0", "--sse-port", "8080", "postgresql://admin:Pass123qp@postgres:5432/postgres"]

    environment:
      - MCP_ALLOW_WRITE=false
    ports:
      # Expose the container's SSE port
      - "48010:8080"
    networks:
      - litellm-mcp-net
      - postgres-net
    healthcheck:
      disable: true

networks:
  litellm-mcp-net:
    external: true
  postgres-net:
    external: true
```

#### Updated LiteLLM Configuration
The LiteLLM `config.yaml` uses SSE transport:
```yaml
mcp_servers:
  db_main:
    transport: sse
    url: http://mcp-postgres:8080/sse
    api_keys: [${LITELLM_VIRTUAL_KEY_TEST}]
    description: "PostgreSQL database tools via native SSE"
    timeout: 30
```

2. Create `/home/administrator/secrets/mcp-postgres.env` containing `DATABASE_URI=postgresql://...` (read-only credentials preferred) and set permissions `chmod 600 /home/administrator/secrets/mcp-postgres.env`.
3. Document service details in `/home/administrator/projects/mcp/postgres/CLAUDE.md` once operational.

### Phase E ‚Äì Deployment & Verification
0. **Create shared network:** `docker network create litellm-mcp-net` (safe to rerun; Docker will warn if it already exists).
1. Pull images: `docker compose pull` within both `projects/litellm` and each MCP directory.
2. Start MCP connectors first (e.g., `docker compose up -d` inside `projects/mcp/postgres`).
3. Start LiteLLM: `docker compose up -d` inside `projects/litellm`.
4. Validate container health:
   ```bash
   docker compose ps
   docker compose logs litellm-proxy --tail 100
   ```
5. Confirm tool discovery:
   ```bash
   curl -s http://linuxserver.lan:4000/v1/models \
     -H "Authorization: Bearer ${LITELLM_VIRTUAL_KEY_TEST}" | jq '.data[] | select(.id=="gpt-4o-mock").tools'
   ```
6. Verify MCP server registration explicitly:
   ```bash
   curl -s http://linuxserver.lan:4000/v1/models \
     -H "Authorization: Bearer ${LITELLM_VIRTUAL_KEY_TEST}" \
     -H "x-mcp-servers: db" | jq '.data[] | select(.id=="gpt-4o-mock").mcp_servers'
   ```
7. Test MCP call via `/v1/responses` (preferred for tool execution):
   ```bash
   curl -s http://linuxserver.lan:4000/v1/responses \
     -H "Authorization: Bearer ${LITELLM_VIRTUAL_KEY_TEST}" \
     -H "Content-Type: application/json" \
     -H "x-mcp-servers: db" \
     -d '{
       "model": "gpt-4o-mock",
       "input": [{"role": "user", "content": [{"type": "text", "text": "List schemas."}]}]
     }' | jq
   ```
8. Verify LiteLLM persisted log entry in Postgres (`SELECT model, user_id, request_tags->'tool_calls' FROM litellm_logs ORDER BY start_time DESC LIMIT 5;`).

### Phase F ‚Äì Client Integration
- **Claude Code CLI / Gemini CLI / ChatGPT Codex CLI:** Point `api_base` to `http://linuxserver.lan:4000/v1`, include `Authorization: Bearer <virtual key>` and `x-mcp-servers` header.
- **Open WebUI:** Configure OpenAI-compatible backend using LiteLLM endpoint; ensure environment allows custom headers for MCP usage.
- **VS Code (MCP extension):** Add LiteLLM entry in `mcp.json` referencing LAN URL and key.
- Document connection instructions in `/home/administrator/projects/litellm/README.md`.

### Phase G ‚Äì Observability, Backups, Hardening
- Promtail auto-discovers containers; ensure labels `logging=enabled` and `project=litellm`.
- Add Grafana dashboard for LiteLLM metrics (request count, latency, tool usage) using Loki queries.
- Backup LiteLLM Postgres schema nightly with existing scripts (`/home/administrator/projects/postgres/backupdb.sh litellm_db`).
- Plan future Traefik/OAuth2 proxy integration (Keycloak) once LAN-only testing is complete.
- Rotate virtual keys periodically; document rotation procedure in `litellm/README.md`.

## 6. Protocol Recipes & Patterns
- **SSE Template:** Provided in Phase D (preferred approach).
- **HTTP Template:** See Section 4 examples; ensure MCP server exposes `/health`.
- **Stdio Template:** Package tool binaries within `/home/administrator/projects/litellm/tools`; update Docker image via overlay or bind mount. Keep usage minimal; prefer SSE wrappers for better isolation.

### HTTP Adapter Pattern (For stdio-only MCP Tools)
**Use Case**: When MCP tools only support stdio transport but you need HTTP/SSE connectivity.

**Architecture**: Client ‚Üí LiteLLM ‚Üí HTTP Adapter ‚Üí stdio MCP Tool

**Implementation Template**:
```javascript
// adapter.js - HTTP-to-stdio bridge pattern
const express = require('express');
const { spawn } = require('child_process');
const app = express();
app.use(express.json());

app.post('/mcp', (req, res) => {
    const mcpProcess = spawn('your-mcp-tool');
    let stdoutData = '';

    mcpProcess.stdout.on('data', (data) => {
        stdoutData += data.toString();
    });

    mcpProcess.on('close', (code) => {
        if (code !== 0) {
            return res.status(500).send(`MCP process exited with code ${code}`);
        }
        try {
            const responses = stdoutData.trim().split('\n');
            const lastResponse = responses[responses.length - 1];
            res.setHeader('Content-Type', 'application/json');
            res.send(lastResponse);
        } catch (e) {
            res.status(500).json({ error: 'Failed to parse MCP response', details: stdoutData });
        }
    });

    mcpProcess.stdin.write(JSON.stringify(req.body) + '\n');
    mcpProcess.stdin.end();
});

app.listen(8080, () => {
    console.log('MCP HTTP adapter running on port 8080');
});
```

**Dockerfile Template**:
```dockerfile
FROM your-mcp-tool-image:latest
USER root
RUN apt-get update && \
    apt-get install -y curl && \
    curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
    apt-get install -y nodejs
USER original-user
WORKDIR /app
COPY adapter.js .
RUN npm install express
CMD ["node", "adapter.js"]
```

**When to Use**: Only when the MCP tool lacks native HTTP/SSE support. Always check for native transport options first (like postgres SSE support).

## 7. Expansion Roadmap
1. Deploy additional MCP services (`filesystem`, `fetch`, `n8n`, `timescaledb`). Each gets its own directory, Compose file, secrets, and LiteLLM `mcp_servers` entry.
2. Replace mock model with real providers once tool pipeline validated; add provider API keys to `litellm.env` and extend `model_list`.
3. Add Traefik labels to LiteLLM container for `litellm.linuxserver.lan` hostname and optional OAuth2 proxy for authenticated external access.
4. Evaluate LiteLLM rate limiting, quotas, and Redis caching if usage increases.
5. Update AINotes docs (`SYSTEM-OVERVIEW.md`, `network.md`, `security.md`) post-deployment per coding standards.

## 8. Implementation Status (2025-09-21 Update)

### ‚úÖ COMPLETED: Environment Variable Fix
**Critical Discovery**: LiteLLM requires `os.environ/VARIABLE_NAME` syntax instead of `${VARIABLE_NAME}` for environment variable substitution.

**Applied to**:
- `litellm_settings.master_key: os.environ/LITELLM_MASTER_KEY`
- `litellm_settings.database_url: os.environ/DATABASE_URL`
- `model_list[].litellm_params.api_key: os.environ/ANTHROPIC_API_KEY`
- `virtual_keys[].api_key: os.environ/LITELLM_VIRTUAL_KEY_TEST`
- `mcp_servers[].api_keys: [os.environ/LITELLM_VIRTUAL_KEY_TEST]`

### ‚úÖ COMPLETED: Database Authentication & Connectivity
- PostgreSQL SCRAM-SHA-256 authentication working
- Database connection string operational: `postgresql://litellm_user:LiteLLMPass2025@postgres:5432/litellm_db`
- Virtual keys manually restored after database recreation

### ‚úÖ COMPLETED: Model Configuration
- Claude-3-haiku-20240307 model healthy and responding
- OpenAI-compatible API endpoints functional
- Master key authentication working

### ‚ùå UNRESOLVED: MCP Function Execution Issue
**Problem**: Function calls generated but not executed
- Claude correctly generates `tool_calls` with proper JSON structure
- Virtual key authentication functional
- MCP postgres service running and receiving connections
- Function calls stop at `"finish_reason":"tool_calls"` without execution
- Missing execution phase of MCP workflow

**Root Cause**: Unknown - appears to be LiteLLM MCP integration issue
- GitHub issue #16688: "MCP tool call parsed, but sometimes not executed"
- Virtual key association with MCP servers may be incomplete
- Execution routing from LiteLLM to MCP servers not functioning

### üîß Current Configuration
```yaml
# Working config.yaml syntax
litellm_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

model_list:
  - model_name: claude-3-haiku-orchestrator
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY

virtual_keys:
  - api_key: os.environ/LITELLM_VIRTUAL_KEY_TEST
    models: ["claude-3-haiku-orchestrator"]
    mcp_servers: ["db_main"]

mcp_servers:
  db_main:
    transport: sse
    url: http://mcp-postgres:8080/sse
    api_keys: [os.environ/LITELLM_VIRTUAL_KEY_TEST]
    require_approval: "never"
```

### üìã Implementation Lessons Learned
1. **Environment Variables**: Use `os.environ/VAR` not `${VAR}` in LiteLLM config
2. **Database Auth**: PostgreSQL requires SCRAM-SHA-256 encryption for passwords
3. **Virtual Keys**: Manual restoration required after database recreation
4. **MCP Execution**: Function generation works, execution phase fails
5. **SSE Transport**: Network connectivity confirmed working between containers

## 9. Risks and Open Questions
- **MCP Execution Architecture**: How LiteLLM routes function calls to MCP servers for execution
- **Virtual Key Association**: Proper database associations between virtual keys and MCP servers
- **Function Execution Trigger**: What triggers execution phase after tool_calls generation
- **LiteLLM MCP Bugs**: Known issues with MCP tool execution in v1.77.3+
- **Alternative Approaches**: Direct MCP client vs LiteLLM proxy gateway

## Executive Summary
LiteLLM Proxy successfully deployed with working authentication, environment variable substitution, and Claude model integration. Critical environment variable syntax discovered (`os.environ/VAR`). Database connectivity and virtual keys restored. **However, MCP function execution remains unresolved** - function calls are generated correctly but not executed by LiteLLM. This appears to be a known issue with LiteLLM MCP integration requiring further investigation or alternative approaches. The infrastructure is in place and partially functional, but the core MCP execution workflow needs resolution before full client integration can proceed.
